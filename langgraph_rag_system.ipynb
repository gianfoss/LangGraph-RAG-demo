{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG System with LangGraph + Local AI\n",
    "\n",
    "This notebook demonstrates a **Retrieval-Augmented Generation (RAG)** system built on top of **LangGraph** that queries a corpus of policy documents stored in a **Weaviate** vector database, with LLM inference powered by a local **Certara.AI** deployment.\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "```\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502                  LangGraph Agent                    \u2502\n",
    "                    \u2502                                                     \u2502\n",
    "  User \u2500\u2500\u2500\u2500\u2500\u2500\u25ba     \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
    "  Question         \u2502  \u2502  Classify  \u2502\u2500\u2500\u2500\u25ba\u2502  Decompose / \u2502\u2500\u2500\u2500\u25ba\u2502 Parallel \u2502  \u2502\n",
    "                   \u2502  \u2502  Intent    \u2502    \u2502  Rephrase     \u2502    \u2502 Retrieve \u2502  \u2502\n",
    "                   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "                   \u2502        \u2502 need info?                         \u2502        \u2502\n",
    "                   \u2502        \u25bc                                    \u25bc        \u2502\n",
    "                   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "                   \u2502  \u2502 Clarify / \u2502\u25c4\u2500\u2500\u2500\u2500 User             \u2502 Generate \u2502   \u2502\n",
    "                   \u2502  \u2502 Ask User  \u2502                       \u2502 Answer   \u2502   \u2502\n",
    "                   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "                   \u2502                                            \u2502        \u2502\n",
    "                   \u2502                                      \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n",
    "                   \u2502                                      \u2502 Check Role \u2502 \u2502\n",
    "                   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \u2502 Ambiguity  \u2502 \u2502\n",
    "                   \u2502  \u2502 Ask Role  \u2502\u25c4\u2500\u2500\u2500 ambiguous \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n",
    "                   \u2502  \u2502 (\u2192 User)  \u2502                          clear\u2502       \u2502\n",
    "                   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510   \u2502\n",
    "                   \u2502                                      \u2502 Evaluate \u2502   \u2502\n",
    "                   \u2502                                      \u2502 Answer   \u2502   \u2502\n",
    "                   \u2502                                      \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "                   \u2502                                     pass / retry    \u2502\n",
    "                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                          \u2502\n",
    "              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "              \u2502                                                       \u2502\n",
    "     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "     \u2502  Certara.AI LLM \u2502  (local OpenAI-compatible API)    \u2502  Weaviate Vector \u2502\n",
    "     \u2502  /v1/chat/...   \u2502                                   \u2502  Database         \u2502\n",
    "     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "## Key Capabilities\n",
    "\n",
    "| # | Capability | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | **Clarity Check** | Detects ambiguous or under-specified questions and asks the user for clarification |\n",
    "| 2 | **Question Decomposition** | Breaks complex questions into sub-queries and runs parallel retrievals |\n",
    "| 3 | **Answer Evaluation & Retry** | Scores the generated answer and retries retrieval if confidence is low |\n",
    "| 4 | **Conversational Follow-up** | Handles multi-turn dialogue, deciding when new retrieval is needed |\n",
    "| 5 | **Role Disambiguation** | Detects when retrieved sources contain role-specific answers and asks the user which role applies |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 \u00b7 Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# %pip install langgraph langchain langchain-openai weaviate-client httpx pydantic --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import httpx\n",
    "import asyncio\n",
    "from typing import TypedDict, Annotated, Literal, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Certara.AI Configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# These point to your local Certara.AI deployment which exposes an\n",
    "# OpenAI-compatible API for text generation.\n",
    "\n",
    "CERTARA_BASE_URL = os.getenv(\"CERTARA_BASE_URL\", \"http://localhost:8000/v1\")\n",
    "CERTARA_API_KEY  = os.getenv(\"CERTARA_API_KEY\", \"certara-local-key\")\n",
    "CERTARA_MODEL    = os.getenv(\"CERTARA_MODEL\", \"certara-default\")\n",
    "\n",
    "# \u2500\u2500 Weaviate Configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# The vector DB that holds chunked policy documents.\n",
    "\n",
    "WEAVIATE_URL        = os.getenv(\"WEAVIATE_URL\", \"http://localhost:8080\")\n",
    "WEAVIATE_COLLECTION = os.getenv(\"WEAVIATE_COLLECTION\", \"PolicyDocuments\")\n",
    "\n",
    "# \u2500\u2500 Agent Parameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "MAX_RETRIEVAL_RETRIES = 2          # Max retry rounds for low-confidence answers\n",
    "CONFIDENCE_THRESHOLD  = 0.6        # Min confidence to accept an answer\n",
    "TOP_K_RESULTS         = 3          # Chunks per sub-query (keep low \u2014 decomposition multiplies this)\n",
    "\n",
    "print(f\"Certara.AI endpoint : {CERTARA_BASE_URL}\")\n",
    "print(f\"Weaviate endpoint   : {WEAVIATE_URL}\")\n",
    "print(f\"Collection          : {WEAVIATE_COLLECTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 \u00b7 API Client Wrappers\n",
    "\n",
    "We wrap the three primary interaction modes with Certara.AI into clean helper functions:\n",
    "\n",
    "| Mode | Endpoint | Purpose |\n",
    "|------|----------|---------|\n",
    "| **Generate** | `POST /v1/chat/completions` | LLM text generation |\n",
    "| **Retrieve** | `POST /v1/retrieve` | Semantic search over Weaviate |\n",
    "| **RAG (combined)** | `POST /v1/rag` | Retrieve + Generate in one call |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 LLM via LangChain (uses the OpenAI-compatible /v1/chat/completions) \u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=CERTARA_BASE_URL,\n",
    "    api_key=CERTARA_API_KEY,\n",
    "    model=CERTARA_MODEL,\n",
    "    temperature=0.1,\n",
    "    timeout=120,          # seconds \u2014 avoids 30s timeout on large contexts\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Quick smoke test (will fail gracefully if Certara.AI is not running)\n",
    "try:\n",
    "    resp = llm.invoke([HumanMessage(content=\"Say hello in one sentence.\")])\n",
    "    print(f\"LLM connected \u2713  Response: {resp.content[:80]}\")\n",
    "except Exception as e:\n",
    "    print(f\"LLM not reachable (expected if Certara.AI is not running): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Weaviate Retrieval Client \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "async def retrieve_from_weaviate(\n",
    "    query: str,\n",
    "    top_k: int = TOP_K_RESULTS,\n",
    "    collection: str = WEAVIATE_COLLECTION,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Mode 2: Retrieve relevant chunks from the Weaviate vector database\n",
    "    via the Certara.AI retrieval endpoint.\n",
    "\n",
    "    Returns a list of dicts with keys: text, source, score, metadata.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"collection\": collection,\n",
    "        \"top_k\": top_k,\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=30) as client:\n",
    "        response = await client.post(\n",
    "            f\"{CERTARA_BASE_URL.rstrip('/v1')}/v1/retrieve\",\n",
    "            json=payload,\n",
    "            headers={\"Authorization\": f\"Bearer {CERTARA_API_KEY}\"},\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "    # Normalize the response into a consistent format\n",
    "    results = []\n",
    "    for item in data.get(\"results\", []):\n",
    "        results.append({\n",
    "            \"text\": item.get(\"text\", item.get(\"content\", \"\")),\n",
    "            \"source\": item.get(\"source\", item.get(\"metadata\", {}).get(\"source\", \"unknown\")),\n",
    "            \"score\": item.get(\"score\", item.get(\"distance\", 0.0)),\n",
    "            \"metadata\": item.get(\"metadata\", {}),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "async def parallel_retrieve(queries: list[str], top_k: int = TOP_K_RESULTS) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Run multiple retrieval queries in parallel and deduplicate results.\n",
    "    \"\"\"\n",
    "    tasks = [retrieve_from_weaviate(q, top_k=top_k) for q in queries]\n",
    "    all_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    # Flatten and deduplicate by text content\n",
    "    seen_texts = set()\n",
    "    deduplicated = []\n",
    "    for result_set in all_results:\n",
    "        if isinstance(result_set, Exception):\n",
    "            print(f\"  \u26a0 Retrieval error: {result_set}\")\n",
    "            continue\n",
    "        for r in result_set:\n",
    "            text_key = r[\"text\"][:200]  # dedup on first 200 chars\n",
    "            if text_key not in seen_texts:\n",
    "                seen_texts.add(text_key)\n",
    "                deduplicated.append(r)\n",
    "\n",
    "    # Sort by relevance score (higher is better)\n",
    "    deduplicated.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return deduplicated\n",
    "\n",
    "\n",
    "print(\"Retrieval functions defined \u2713\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Combined RAG Client \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "async def certara_rag_call(\n",
    "    query: str,\n",
    "    top_k: int = TOP_K_RESULTS,\n",
    "    system_prompt: str = \"You are a helpful policy assistant.\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Mode 3: Combined Retrieve + Generate call to Certara.AI.\n",
    "    The server handles both retrieval and generation in a single request.\n",
    "\n",
    "    Returns: {\"answer\": str, \"sources\": list[dict]}\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"collection\": WEAVIATE_COLLECTION,\n",
    "        \"top_k\": top_k,\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"model\": CERTARA_MODEL,\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=60) as client:\n",
    "        response = await client.post(\n",
    "            f\"{CERTARA_BASE_URL.rstrip('/v1')}/v1/rag\",\n",
    "            json=payload,\n",
    "            headers={\"Authorization\": f\"Bearer {CERTARA_API_KEY}\"},\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "print(\"RAG client defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 \u00b7 Graph State Definition\n",
    "\n",
    "LangGraph uses a typed state object that flows through every node in the graph. Our state tracks the full conversation history, retrieved sources, decomposed sub-queries, confidence scores, and retry counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(TypedDict):\n",
    "    \"\"\"State that flows through the LangGraph RAG pipeline.\"\"\"\n",
    "\n",
    "    # \u2500\u2500 Conversation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    messages: Annotated[list[BaseMessage], add_messages]  # full chat history\n",
    "    current_query: str                                     # latest user question\n",
    "\n",
    "    # \u2500\u2500 Intent Classification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    intent: str              # \"clear_question\" | \"needs_clarification\" | \"follow_up\" | \"role_response\" | \"chitchat\"\n",
    "    needs_retrieval: bool    # whether follow-up requires new retrieval\n",
    "\n",
    "    # \u2500\u2500 Decomposition & Retrieval \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    sub_queries: list[str]           # decomposed / rephrased queries\n",
    "    retrieved_sources: list[dict]    # chunks from Weaviate\n",
    "\n",
    "    # \u2500\u2500 Generation & Evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    draft_answer: str        # generated answer (before evaluation)\n",
    "    confidence: float        # 0.0 \u2013 1.0 self-assessed confidence\n",
    "    retry_count: int         # number of retrieval retries so far\n",
    "    final_answer: str        # accepted answer to present to user\n",
    "\n",
    "    # \u2500\u2500 Role Disambiguation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    detected_roles: list[str]   # roles found in sources (e.g. [\"manager\", \"IC\", \"contractor\"])\n",
    "    role_ambiguous: bool        # True if answer differs by role and user hasn't specified\n",
    "    user_role: str              # role the user identifies with (set after clarification)\n",
    "    pending_query: str           # original question saved when asking for role clarification\n",
    "\n",
    "\n",
    "print(\"State schema defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 \u00b7 Graph Nodes\n",
    "\n",
    "Each node is an async function that reads state, performs work, and returns a partial state update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 \u2014 Classify Intent\n",
    "\n",
    "The first node examines the user's message in context of the conversation history and classifies it as:\n",
    "- **`clear_question`** \u2014 well-formed policy question \u2192 proceed to decomposition  \n",
    "- **`needs_clarification`** \u2014 ambiguous or missing information \u2192 ask user  \n",
    "- **`follow_up`** \u2014 continuation of a prior topic \u2192 check if new retrieval is needed  \n",
    "- **`chitchat`** \u2014 greeting or off-topic \u2192 respond directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFY_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an intent classifier for a policy Q&A system.\n",
    "\n",
    "Given the conversation history and the latest user message, classify the intent\n",
    "into EXACTLY ONE of the following categories and respond with valid JSON only:\n",
    "\n",
    "{{\n",
    "  \"intent\": \"clear_question\" | \"needs_clarification\" | \"follow_up\" | \"role_response\" | \"chitchat\",\n",
    "  \"needs_retrieval\": true | false,\n",
    "  \"user_role\": \"<extracted role or empty string>\",\n",
    "  \"reasoning\": \"<one sentence explaining your classification>\"\n",
    "}}\n",
    "\n",
    "Guidelines:\n",
    "- \"clear_question\": The user asks a specific, answerable question about policy.\n",
    "- \"needs_clarification\": The question is too vague, ambiguous, or references\n",
    "  something unspecified (e.g., \"What's the policy?\" without saying WHICH policy,\n",
    "  or \"Tell me about the rules\" without context).\n",
    "- \"follow_up\": The user is continuing a previous topic \u2014 asking for more detail,\n",
    "  rephrasing, or adding a related question. Set needs_retrieval=true if answering\n",
    "  likely requires new information beyond what was already retrieved.\n",
    "- \"role_response\": The assistant previously asked the user to specify their\n",
    "  organizational role/group, and the user is now providing that information\n",
    "  (e.g., \"I'm a manager\", \"contractor\", \"I'm on the engineering team\").\n",
    "  IMPORTANT: You MUST extract the role into the user_role field.\n",
    "- \"chitchat\": Greetings, thanks, or off-topic messages.\n",
    "\n",
    "Respond ONLY with the JSON object, no other text.\"\"\"),\n",
    "    (\"human\", \"\"\"Conversation so far:\n",
    "{history}\n",
    "\n",
    "Latest user message: {query}\"\"\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def classify_intent(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Classify the user's intent.\"\"\"\n",
    "    # Build a readable history string\n",
    "    history_lines = []\n",
    "    for msg in state.get(\"messages\", [])[:-1]:  # exclude latest\n",
    "        role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "        history_lines.append(f\"{role}: {msg.content}\")\n",
    "    history_str = \"\\n\".join(history_lines[-10:]) or \"(no prior conversation)\"\n",
    "\n",
    "    chain = CLASSIFY_PROMPT | llm | StrOutputParser()\n",
    "    raw = await chain.ainvoke({\"history\": history_str, \"query\": state[\"current_query\"]})\n",
    "\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        parsed = {\"intent\": \"clear_question\", \"needs_retrieval\": True, \"reasoning\": \"Parse error, defaulting.\"}\n",
    "\n",
    "    # \u2500\u2500 Robust role extraction \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # If the LLM classified this as role_response but omitted user_role\n",
    "    # (common LLM failure), fall back to using the raw user message as the\n",
    "    # role. This is safe because role_response only triggers when the prior\n",
    "    # assistant message was a role-clarification question.\n",
    "    user_role = parsed.get(\"user_role\", \"\").strip()\n",
    "    if parsed.get(\"intent\") == \"role_response\" and not user_role:\n",
    "        user_role = state[\"current_query\"].strip()\n",
    "        print(f\"     \u26a0 LLM omitted user_role \u2014 using raw message: \\\"{user_role}\\\"\")\n",
    "\n",
    "    # Preserve role from prior turns if the LLM didn't extract a new one\n",
    "    if not user_role:\n",
    "        user_role = state.get(\"user_role\", \"\")\n",
    "\n",
    "    # \u2500\u2500 Restore original question for role responses \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # When the user says \"I'm a contractor\", current_query is that role\n",
    "    # statement \u2014 not the original policy question. We need to swap it back\n",
    "    # so that generate_answer receives the real question.\n",
    "    result = {\n",
    "        \"intent\": parsed[\"intent\"],\n",
    "        \"needs_retrieval\": parsed.get(\"needs_retrieval\", True),\n",
    "        \"user_role\": user_role,\n",
    "    }\n",
    "\n",
    "    if parsed.get(\"intent\") == \"role_response\":\n",
    "        original_q = state.get(\"pending_query\", \"\")\n",
    "        if original_q:\n",
    "            result[\"current_query\"] = original_q\n",
    "            print(f\"     \ud83d\udd04 Restored original query: \\\"{original_q}\\\"\")\n",
    "        else:\n",
    "            print(\"     \u26a0 No pending_query found \u2014 generate will use role message as query\")\n",
    "\n",
    "    print(f\"  \ud83c\udff7  Intent: {parsed['intent']}  |  Needs retrieval: {parsed.get('needs_retrieval', True)}\")\n",
    "    print(f\"     Reasoning: {parsed.get('reasoning', '')}\")\n",
    "    if user_role:\n",
    "        print(f\"     \ud83d\udc64 User role: {user_role}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"classify_intent node defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 \u2014 Clarify (Ask User for More Information)\n",
    "\n",
    "When the question is too vague, this node generates a clarification request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLARIFY_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful policy assistant. The user's question is\n",
    "ambiguous or lacks sufficient detail for you to look up an answer in the policy\n",
    "database. Write a concise, friendly message asking the user to clarify.\n",
    "Be specific about WHAT information is missing.\"\"\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def clarify(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Ask the user for clarification.\"\"\"\n",
    "    chain = CLARIFY_PROMPT | llm | StrOutputParser()\n",
    "    clarification = await chain.ainvoke({\"query\": state[\"current_query\"]})\n",
    "\n",
    "    print(f\"  \u2753 Asking for clarification\")\n",
    "\n",
    "    return {\n",
    "        \"final_answer\": clarification,\n",
    "        \"messages\": [AIMessage(content=clarification)],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"clarify node defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 \u2014 Decompose & Rephrase Queries\n",
    "\n",
    "Complex questions are broken into focused sub-queries that are individually optimized for vector-similarity retrieval. This improves recall significantly over sending a single long question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECOMPOSE_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a query decomposition engine for a policy document search system.\n",
    "\n",
    "Given the user's question (and conversation history for context), produce 1\u20134\n",
    "focused search queries that together cover the full information need.\n",
    "\n",
    "Rules:\n",
    "- Each sub-query should be a short, specific phrase optimized for semantic search.\n",
    "- Replace pronouns with their referents using conversation context.\n",
    "- If the question is already simple and specific, return it as a single query.\n",
    "- Do NOT add queries for information the user did not ask about.\n",
    "\n",
    "Respond with valid JSON only:\n",
    "{{\"queries\": [\"query 1\", \"query 2\", ...]}}\"\"\"),\n",
    "    (\"human\", \"\"\"Conversation context:\n",
    "{history}\n",
    "\n",
    "Current question: {query}\"\"\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def decompose_query(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Decompose / rephrase the question into retrieval-optimized sub-queries.\"\"\"\n",
    "    history_lines = []\n",
    "    for msg in state.get(\"messages\", [])[:-1]:\n",
    "        role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "        history_lines.append(f\"{role}: {msg.content}\")\n",
    "    history_str = \"\\n\".join(history_lines[-6:]) or \"(none)\"\n",
    "\n",
    "    chain = DECOMPOSE_PROMPT | llm | StrOutputParser()\n",
    "    raw = await chain.ainvoke({\"history\": history_str, \"query\": state[\"current_query\"]})\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        sub_queries = parsed.get(\"queries\", [state[\"current_query\"]])\n",
    "    except json.JSONDecodeError:\n",
    "        sub_queries = [state[\"current_query\"]]\n",
    "\n",
    "    print(f\"  \ud83d\udd00 Decomposed into {len(sub_queries)} sub-queries:\")\n",
    "    for i, q in enumerate(sub_queries, 1):\n",
    "        print(f\"     {i}. {q}\")\n",
    "\n",
    "    return {\"sub_queries\": sub_queries}\n",
    "\n",
    "\n",
    "print(\"decompose_query node defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 \u2014 Parallel Retrieval\n",
    "\n",
    "All sub-queries are dispatched to Weaviate **in parallel** via `asyncio.gather`. Results are deduplicated and sorted by relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Run parallel retrieval for all sub-queries against Weaviate.\"\"\"\n",
    "    queries = state.get(\"sub_queries\", [state[\"current_query\"]])\n",
    "\n",
    "    print(f\"  \ud83d\udd0d Retrieving from Weaviate ({len(queries)} parallel queries)...\")\n",
    "    sources = await parallel_retrieve(queries, top_k=TOP_K_RESULTS)\n",
    "\n",
    "    # Merge with any previously retrieved sources (for retries)\n",
    "    existing = state.get(\"retrieved_sources\", [])\n",
    "    existing_texts = {s[\"text\"][:200] for s in existing}\n",
    "    for s in sources:\n",
    "        if s[\"text\"][:200] not in existing_texts:\n",
    "            existing.append(s)\n",
    "\n",
    "    print(f\"  \ud83d\udcc4 Retrieved {len(sources)} new chunks ({len(existing)} total)\")\n",
    "\n",
    "    return {\"retrieved_sources\": existing}\n",
    "\n",
    "\n",
    "print(\"retrieve node defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 \u2014 Generate Answer\n",
    "\n",
    "The generation node assembles retrieved context into a prompt and calls the LLM to produce a grounded answer with citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a policy expert assistant. Answer the user's question\n",
    "using ONLY the provided source documents. Follow these rules:\n",
    "\n",
    "1. Base your answer strictly on the retrieved sources.\n",
    "2. Cite sources using [Source: <filename>] notation.\n",
    "3. If the sources do not contain enough information, say so clearly.\n",
    "4. Be concise but thorough.\n",
    "5. If multiple sources conflict, note the discrepancy.\n",
    "6. If a user role/group is specified below, tailor your answer specifically\n",
    "   to that role. Only include information relevant to their role and\n",
    "   explicitly state which role the answer applies to.\n",
    "\n",
    "User's organizational role: {user_role}\n",
    "\n",
    "Retrieved Sources:\n",
    "{context}\"\"\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def generate_answer(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Generate an answer grounded in retrieved sources.\n",
    "\n",
    "    Applies a token budget to avoid exceeding the Certara.AI context\n",
    "    window (~30k tokens) or triggering a response timeout.\n",
    "    If user_role is set, the prompt instructs the LLM to tailor the\n",
    "    answer for that specific role.\n",
    "    \"\"\"\n",
    "    sources = state.get(\"retrieved_sources\", [])\n",
    "    user_role = state.get(\"user_role\", \"\") or \"(not specified)\"\n",
    "\n",
    "    if user_role != \"(not specified)\":\n",
    "        print(f\"  \ud83d\udc64 Generating answer tailored to role: {user_role}\")\n",
    "\n",
    "    # \u2500\u2500 Token budget management \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # Reserve tokens for: system prompt (~300), query (~100), generation (~2k)\n",
    "    # Rough estimate: 1 token \u2248 4 characters\n",
    "    MAX_CONTEXT_CHARS = 60_000   # ~15k tokens, well within 30k limit\n",
    "    MAX_CHUNKS = 6               # Hard cap on chunks to include\n",
    "\n",
    "    context_parts = []\n",
    "    total_chars = 0\n",
    "    for i, s in enumerate(sources[:MAX_CHUNKS], 1):\n",
    "        chunk_text = (\n",
    "            f\"--- Source {i} [Source: {s['source']}] \"\n",
    "            f\"(score: {s['score']:.3f}) ---\\n{s['text']}\"\n",
    "        )\n",
    "        if total_chars + len(chunk_text) > MAX_CONTEXT_CHARS:\n",
    "            print(f\"  \u26a0 Context budget reached at {i-1} chunks ({total_chars} chars)\")\n",
    "            break\n",
    "        context_parts.append(chunk_text)\n",
    "        total_chars += len(chunk_text)\n",
    "\n",
    "    context_str = \"\\n\\n\".join(context_parts) or \"(No sources retrieved)\"\n",
    "    print(f\"  \ud83d\udcd0 Context: {len(context_parts)} chunks, ~{total_chars // 4} tokens\")\n",
    "\n",
    "    chain = GENERATE_PROMPT | llm | StrOutputParser()\n",
    "    answer = await chain.ainvoke({\n",
    "        \"context\": context_str,\n",
    "        \"query\": state[\"current_query\"],\n",
    "        \"user_role\": user_role,\n",
    "    })\n",
    "\n",
    "    print(f\"  \u270d  Draft answer generated ({len(answer)} chars)\")\n",
    "\n",
    "    return {\"draft_answer\": answer}\n",
    "\n",
    "\n",
    "print(\"generate_answer node defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 \u2014 Evaluate Answer\n",
    "\n",
    "The evaluation node uses a separate LLM call to score the draft answer for:\n",
    "- **Relevance**: Does it address what was asked?\n",
    "- **Groundedness**: Is it supported by the retrieved sources?\n",
    "- **Completeness**: Does it fully answer the question?\n",
    "\n",
    "If confidence is below the threshold, the system retries with refined queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATE_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a critical evaluator for a RAG system. Assess the quality\n",
    "of the generated answer given the original question and the retrieved sources.\n",
    "\n",
    "Score each dimension from 0.0 to 1.0:\n",
    "- relevance: Does the answer address the question?\n",
    "- groundedness: Is every claim supported by the sources?\n",
    "- completeness: Does the answer fully address all parts of the question?\n",
    "\n",
    "Also suggest what additional information to search for if the answer is incomplete.\n",
    "\n",
    "Respond with valid JSON only:\n",
    "{{\n",
    "  \"relevance\": 0.0-1.0,\n",
    "  \"groundedness\": 0.0-1.0,\n",
    "  \"completeness\": 0.0-1.0,\n",
    "  \"overall_confidence\": 0.0-1.0,\n",
    "  \"missing_info\": \"description of what's missing, or 'none'\",\n",
    "  \"retry_queries\": [\"suggested search query 1\", ...]\n",
    "}}\"\"\"),\n",
    "    (\"human\", \"\"\"Question: {query}\n",
    "\n",
    "Generated Answer:\n",
    "{answer}\n",
    "\n",
    "Sources used (count): {source_count}\"\"\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def evaluate_answer(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Evaluate the draft answer and decide whether to accept or retry.\"\"\"\n",
    "    chain = EVALUATE_PROMPT | llm | StrOutputParser()\n",
    "    raw = await chain.ainvoke({\n",
    "        \"query\": state[\"current_query\"],\n",
    "        \"answer\": state[\"draft_answer\"],\n",
    "        \"source_count\": len(state.get(\"retrieved_sources\", [])),\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        evaluation = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        evaluation = {\"overall_confidence\": 0.7, \"retry_queries\": []}\n",
    "\n",
    "    confidence = evaluation.get(\"overall_confidence\", 0.7)\n",
    "    retry_queries = evaluation.get(\"retry_queries\", [])\n",
    "\n",
    "    print(f\"  \ud83d\udcca Evaluation \u2014 Confidence: {confidence:.2f}\")\n",
    "    print(f\"     Relevance:    {evaluation.get('relevance', 'N/A')}\")\n",
    "    print(f\"     Groundedness: {evaluation.get('groundedness', 'N/A')}\")\n",
    "    print(f\"     Completeness: {evaluation.get('completeness', 'N/A')}\")\n",
    "    if evaluation.get(\"missing_info\", \"none\") != \"none\":\n",
    "        print(f\"     Missing info: {evaluation['missing_info']}\")\n",
    "\n",
    "    return {\n",
    "        \"confidence\": confidence,\n",
    "        \"sub_queries\": retry_queries if confidence < CONFIDENCE_THRESHOLD else [],\n",
    "        \"retry_count\": state.get(\"retry_count\", 0) + (1 if confidence < CONFIDENCE_THRESHOLD else 0),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"evaluate_answer node defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 \u2014 Finalize Answer\n",
    "\n",
    "Once the answer passes evaluation (or max retries are exhausted), this node packages the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def finalize_answer(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Accept the draft answer as final and add it to the message history.\"\"\"\n",
    "    answer = state[\"draft_answer\"]\n",
    "\n",
    "    # If confidence was low and retries exhausted, prepend a caveat\n",
    "    if state.get(\"confidence\", 1.0) < CONFIDENCE_THRESHOLD:\n",
    "        caveat = (\n",
    "            \"\u26a0\ufe0f **Note:** I was unable to find highly relevant sources for all \"\n",
    "            \"parts of your question. The answer below is based on the best \"\n",
    "            \"available information, but may be incomplete.\\n\\n\"\n",
    "        )\n",
    "        answer = caveat + answer\n",
    "\n",
    "    # Append source citations summary\n",
    "    sources = state.get(\"retrieved_sources\", [])\n",
    "    if sources:\n",
    "        unique_sources = sorted(set(s[\"source\"] for s in sources))\n",
    "        source_list = \"\\n\".join(f\"  \u2022 {s}\" for s in unique_sources)\n",
    "        answer += f\"\\n\\n---\\n**Sources consulted:**\\n{source_list}\"\n",
    "\n",
    "    print(f\"  \u2705 Answer finalized\")\n",
    "\n",
    "    return {\n",
    "        \"final_answer\": answer,\n",
    "        \"messages\": [AIMessage(content=answer)],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"finalize_answer node defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 \u2014 Handle Chitchat\n",
    "\n",
    "Simple conversational responses that don't require retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_chitchat(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Respond to non-policy chitchat (greetings, thanks, etc.).\"\"\"\n",
    "    response = await llm.ainvoke([\n",
    "        SystemMessage(content=(\n",
    "            \"You are a friendly policy assistant. The user sent a casual message \"\n",
    "            \"(greeting, thanks, etc.). Respond briefly and warmly, and let them \"\n",
    "            \"know you're here to help with policy questions.\"\n",
    "        )),\n",
    "        HumanMessage(content=state[\"current_query\"]),\n",
    "    ])\n",
    "\n",
    "    print(f\"  \ud83d\udcac Chitchat response\")\n",
    "\n",
    "    return {\n",
    "        \"final_answer\": response.content,\n",
    "        \"messages\": [AIMessage(content=response.content)],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"handle_chitchat node defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 \u2014 Check Role Ambiguity\n",
    "\n",
    "After generating a draft answer, this node inspects the retrieved sources and the answer to detect whether the response conflates information meant for **different organizational roles** (e.g., manager vs. individual contributor vs. contractor). If multiple role-specific answers exist and the user hasn't indicated their role, the system asks for clarification before finalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_CHECK_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a quality-control reviewer for a policy Q&A system.\n",
    "\n",
    "The organization has many different roles and groups (e.g., full-time employees,\n",
    "contractors, managers, individual contributors, executives, interns, part-time\n",
    "staff, etc.). Some policies and processes differ depending on which group\n",
    "someone belongs to.\n",
    "\n",
    "Review the generated answer AND the source documents below. Determine whether\n",
    "the answer contains information that varies by role/group AND the user has NOT\n",
    "already specified which role applies to them.\n",
    "\n",
    "Respond with valid JSON only:\n",
    "{{\n",
    "  \"role_ambiguous\": true | false,\n",
    "  \"detected_roles\": [\"role1\", \"role2\", ...],\n",
    "  \"reasoning\": \"Brief explanation of why roles are or are not ambiguous\"\n",
    "}}\n",
    "\n",
    "Set role_ambiguous=true ONLY when ALL of these conditions are met:\n",
    "1. The sources contain MATERIALLY DIFFERENT answers depending on role/group\n",
    "   (not just minor wording differences \u2014 the actual policy, process, or\n",
    "   entitlement must differ).\n",
    "2. The user's question does not already specify or imply their role.\n",
    "3. Knowing the role would change the answer the user receives.\n",
    "\n",
    "If the user has already stated their role in the conversation, or if the\n",
    "policy is the same across all roles, set role_ambiguous=false.\"\"\"),\n",
    "    (\"human\", \"\"\"User question: {query}\n",
    "\n",
    "User's stated role (if any): {user_role}\n",
    "\n",
    "Conversation context:\n",
    "{history}\n",
    "\n",
    "Generated answer:\n",
    "{answer}\n",
    "\n",
    "Source excerpts:\n",
    "{sources}\"\"\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def check_role_ambiguity(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Detect if the answer conflates role-specific policy information.\n",
    "\n",
    "    Wraps the entire LLM call in try/except so that any failure\n",
    "    (timeout, parse error, etc.) results in a deterministic state\n",
    "    update rather than leaving role_ambiguous unset.\n",
    "    \"\"\"\n",
    "    # \u2500\u2500 Early exit: role already known \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    user_role = state.get(\"user_role\", \"\")\n",
    "    if user_role:\n",
    "        print(f\"  \ud83d\udc64 User role already known: {user_role} \u2014 skipping role check\")\n",
    "        return {\"role_ambiguous\": False}\n",
    "\n",
    "    # \u2500\u2500 Guard: no sources to check \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    sources = state.get(\"retrieved_sources\", [])\n",
    "    if not sources:\n",
    "        print(\"  \ud83d\udc64 No sources to check for role ambiguity \u2014 skipping\")\n",
    "        return {\"role_ambiguous\": False, \"detected_roles\": []}\n",
    "\n",
    "    # \u2500\u2500 LLM-based role ambiguity detection \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    try:\n",
    "        history_lines = []\n",
    "        for msg in state.get(\"messages\", [])[:-1]:\n",
    "            role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "            history_lines.append(f\"{role}: {msg.content}\")\n",
    "        history_str = \"\\n\".join(history_lines[-6:]) or \"(none)\"\n",
    "\n",
    "        source_excerpts = []\n",
    "        for j, s in enumerate(sources[:6], 1):\n",
    "            source_excerpts.append(f\"[{j}] {s['source']}: {s['text'][:300]}\")\n",
    "        sources_str = \"\\n\".join(source_excerpts)\n",
    "\n",
    "        chain = ROLE_CHECK_PROMPT | llm | StrOutputParser()\n",
    "        raw = await chain.ainvoke({\n",
    "            \"query\": state[\"current_query\"],\n",
    "            \"user_role\": \"(not specified)\",\n",
    "            \"history\": history_str,\n",
    "            \"answer\": state.get(\"draft_answer\", \"\"),\n",
    "            \"sources\": sources_str,\n",
    "        })\n",
    "\n",
    "        # \u2500\u2500 Parse with fallback \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "        try:\n",
    "            parsed = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            # If the LLM returned non-JSON, try to detect role keywords\n",
    "            # in the raw response as a heuristic.\n",
    "            raw_lower = raw.lower()\n",
    "            has_role_signals = any(\n",
    "                kw in raw_lower\n",
    "                for kw in [\"true\", \"ambiguous\", \"different role\", \"varies by\"]\n",
    "            )\n",
    "            print(f\"  \u26a0 JSON parse failed \u2014 heuristic ambiguity={has_role_signals}\")\n",
    "            parsed = {\n",
    "                \"role_ambiguous\": has_role_signals,\n",
    "                \"detected_roles\": [],\n",
    "                \"reasoning\": \"JSON parse error; used heuristic fallback\",\n",
    "            }\n",
    "\n",
    "        # Handle string \"true\"/\"false\" from some LLMs\n",
    "        ambig_raw = parsed.get(\"role_ambiguous\", False)\n",
    "        if isinstance(ambig_raw, str):\n",
    "            ambiguous = ambig_raw.lower().strip() == \"true\"\n",
    "        else:\n",
    "            ambiguous = bool(ambig_raw)\n",
    "\n",
    "        detected = parsed.get(\"detected_roles\", [])\n",
    "\n",
    "    except Exception as e:\n",
    "        # Any other failure (timeout, connection error): safe default\n",
    "        print(f\"  \u26a0 Role check failed ({e}) \u2014 defaulting to no ambiguity\")\n",
    "        ambiguous = False\n",
    "        detected = []\n",
    "\n",
    "    if ambiguous:\n",
    "        print(f\"  \u26a0\ufe0f  Role ambiguity detected \u2014 roles: {detected}\")\n",
    "    else:\n",
    "        print(f\"  \ud83d\udc64 No role ambiguity\")\n",
    "\n",
    "    return {\n",
    "        \"role_ambiguous\": ambiguous,\n",
    "        \"detected_roles\": detected,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"check_role_ambiguity node defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 \u2014 Ask for Role Clarification\n",
    "\n",
    "When role ambiguity is detected, this node generates a targeted question listing the specific roles found in the sources, so the user can indicate which one applies to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASK_ROLE_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful policy assistant. The user asked a question\n",
    "but the answer depends on their organizational role or group. The retrieved\n",
    "policy documents contain different rules for different roles.\n",
    "\n",
    "Write a concise, friendly message that:\n",
    "1. Acknowledges you found relevant information.\n",
    "2. Explains that the answer differs depending on role.\n",
    "3. Lists the specific roles/groups detected and asks the user to indicate\n",
    "   which one applies to them.\n",
    "4. Optionally gives a brief preview of how the answers differ (1 sentence\n",
    "   per role, max) so the user understands why it matters.\n",
    "\n",
    "Keep it concise \u2014 no more than a short paragraph plus the role list.\"\"\"),\n",
    "    (\"human\", \"\"\"User's question: {query}\n",
    "\n",
    "Detected roles with different policies: {roles}\n",
    "\n",
    "Draft answer (for context on how answers differ):\n",
    "{answer}\"\"\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def ask_role_clarification(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Ask the user which organizational role applies to them.\n",
    "\n",
    "    Saves the current question into pending_query so that when the user\n",
    "    responds with their role, we can restore it as the generation target.\n",
    "    \"\"\"\n",
    "    chain = ASK_ROLE_PROMPT | llm | StrOutputParser()\n",
    "    clarification = await chain.ainvoke({\n",
    "        \"query\": state[\"current_query\"],\n",
    "        \"roles\": \", \".join(state.get(\"detected_roles\", [])),\n",
    "        \"answer\": state.get(\"draft_answer\", \"\"),\n",
    "    })\n",
    "\n",
    "    print(f\"  \ud83c\udfad Asking user to specify their role\")\n",
    "    print(f\"     Saved pending query: \\\"{state['current_query']}\\\"\")\n",
    "\n",
    "    return {\n",
    "        \"final_answer\": clarification,\n",
    "        \"messages\": [AIMessage(content=clarification)],\n",
    "        \"pending_query\": state[\"current_query\"],  # preserve for after role response\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"ask_role_clarification node defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 \u00b7 Routing Logic (Conditional Edges)\n",
    "\n",
    "LangGraph uses conditional edges to decide which node to visit next. These functions implement our routing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_classification(state: RAGState) -> str:\n",
    "    \"\"\"\n",
    "    After classifying intent, decide the next step:\n",
    "      - needs_clarification \u2192 ask user\n",
    "      - chitchat \u2192 respond directly\n",
    "      - follow_up (no retrieval needed) \u2192 generate from existing sources\n",
    "      - clear_question / follow_up (retrieval needed) \u2192 decompose & retrieve\n",
    "    \"\"\"\n",
    "    intent = state.get(\"intent\", \"clear_question\")\n",
    "\n",
    "    if intent == \"needs_clarification\":\n",
    "        return \"clarify\"\n",
    "    elif intent == \"chitchat\":\n",
    "        return \"handle_chitchat\"\n",
    "    elif intent == \"role_response\":\n",
    "        # User told us their role \u2014 regenerate answer using existing sources\n",
    "        return \"generate_answer\"\n",
    "    elif intent == \"follow_up\" and not state.get(\"needs_retrieval\", True):\n",
    "        # Follow-up that can be answered from existing context\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        # clear_question or follow_up needing retrieval\n",
    "        return \"decompose_query\"\n",
    "\n",
    "\n",
    "def route_after_evaluation(state: RAGState) -> str:\n",
    "    \"\"\"\n",
    "    After evaluating the answer, decide whether to:\n",
    "      - Accept the answer (confidence >= threshold or retries exhausted)\n",
    "      - Retry with new queries (confidence < threshold and retries remaining)\n",
    "    \"\"\"\n",
    "    confidence = state.get(\"confidence\", 1.0)\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "\n",
    "    if confidence >= CONFIDENCE_THRESHOLD:\n",
    "        print(f\"  \u2705 Confidence {confidence:.2f} >= {CONFIDENCE_THRESHOLD} \u2192 accepting\")\n",
    "        return \"finalize_answer\"\n",
    "    elif retry_count >= MAX_RETRIEVAL_RETRIES:\n",
    "        print(f\"  \u26a0\ufe0f  Max retries ({MAX_RETRIEVAL_RETRIES}) reached \u2192 accepting with caveat\")\n",
    "        return \"finalize_answer\"\n",
    "    else:\n",
    "        print(f\"  \ud83d\udd04 Confidence {confidence:.2f} < {CONFIDENCE_THRESHOLD} \u2192 retrying (attempt {retry_count})\")\n",
    "        return \"retrieve\"  # retry with the new sub_queries set by evaluate\n",
    "\n",
    "\n",
    "def route_after_role_check(state: RAGState) -> str:\n",
    "    \"\"\"\n",
    "    After checking for role ambiguity, decide whether to:\n",
    "      - Ask the user which role applies (ambiguous)\n",
    "      - Continue to answer evaluation (clear / role already known)\n",
    "    \"\"\"\n",
    "    if state.get(\"role_ambiguous\", False):\n",
    "        print(f\"  \ud83c\udfad Role ambiguity \u2192 asking user\")\n",
    "        return \"ask_role_clarification\"\n",
    "    else:\n",
    "        return \"evaluate_answer\"\n",
    "\n",
    "\n",
    "print(\"Routing functions defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 \u00b7 Build the LangGraph\n",
    "\n",
    "Now we assemble all nodes and edges into the complete graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_graph() -> StateGraph:\n",
    "    \"\"\"Construct the full RAG agent graph.\"\"\"\n",
    "\n",
    "    graph = StateGraph(RAGState)\n",
    "\n",
    "    # \u2500\u2500 Add nodes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    graph.add_node(\"classify_intent\",        classify_intent)\n",
    "    graph.add_node(\"clarify\",                clarify)\n",
    "    graph.add_node(\"handle_chitchat\",        handle_chitchat)\n",
    "    graph.add_node(\"decompose_query\",        decompose_query)\n",
    "    graph.add_node(\"retrieve\",               retrieve)\n",
    "    graph.add_node(\"generate_answer\",        generate_answer)\n",
    "    graph.add_node(\"check_role_ambiguity\",   check_role_ambiguity)\n",
    "    graph.add_node(\"ask_role_clarification\", ask_role_clarification)\n",
    "    graph.add_node(\"evaluate_answer\",        evaluate_answer)\n",
    "    graph.add_node(\"finalize_answer\",        finalize_answer)\n",
    "\n",
    "    # \u2500\u2500 Entry point \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    graph.set_entry_point(\"classify_intent\")\n",
    "\n",
    "    # \u2500\u2500 Conditional: after classification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    graph.add_conditional_edges(\n",
    "        \"classify_intent\",\n",
    "        route_after_classification,\n",
    "        {\n",
    "            \"clarify\":          \"clarify\",\n",
    "            \"handle_chitchat\":  \"handle_chitchat\",\n",
    "            \"decompose_query\":  \"decompose_query\",\n",
    "            \"generate_answer\":  \"generate_answer\",  # follow-up or role_response\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # \u2500\u2500 Linear edges \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    graph.add_edge(\"decompose_query\", \"retrieve\")\n",
    "    graph.add_edge(\"retrieve\",        \"generate_answer\")\n",
    "    graph.add_edge(\"generate_answer\", \"check_role_ambiguity\")\n",
    "\n",
    "    # \u2500\u2500 Conditional: after role ambiguity check \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    graph.add_conditional_edges(\n",
    "        \"check_role_ambiguity\",\n",
    "        route_after_role_check,\n",
    "        {\n",
    "            \"ask_role_clarification\": \"ask_role_clarification\",\n",
    "            \"evaluate_answer\":        \"evaluate_answer\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # \u2500\u2500 Conditional: after evaluation (accept vs retry) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    graph.add_conditional_edges(\n",
    "        \"evaluate_answer\",\n",
    "        route_after_evaluation,\n",
    "        {\n",
    "            \"finalize_answer\": \"finalize_answer\",\n",
    "            \"retrieve\":        \"retrieve\",   # retry loop\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # \u2500\u2500 Terminal edges \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    graph.add_edge(\"clarify\",                END)\n",
    "    graph.add_edge(\"handle_chitchat\",        END)\n",
    "    graph.add_edge(\"ask_role_clarification\", END)  # wait for user role response\n",
    "    graph.add_edge(\"finalize_answer\",        END)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "# Build and compile with memory for multi-turn conversations\n",
    "memory = MemorySaver()\n",
    "rag_graph = build_rag_graph()\n",
    "rag_app = rag_graph.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Graph compiled \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 \u2014 Visualize the Graph\n",
    "\n",
    "LangGraph can render a visual representation of the state machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph structure (requires graphviz or mermaid support)\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    img_bytes = rag_app.get_graph().draw_mermaid_png()\n",
    "    display(Image(img_bytes))\n",
    "except Exception as e:\n",
    "    # Fallback: print the mermaid diagram as text\n",
    "    print(\"Graph visualization (Mermaid format):\")\n",
    "    print(rag_app.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 \u00b7 Conversation Runner\n",
    "\n",
    "A helper function that manages the interaction loop \u2014 feeding user messages into the graph, displaying results, and maintaining session state across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask(\n",
    "    question: str,\n",
    "    thread_id: str = \"default-session\",\n",
    "    verbose: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Send a question to the RAG agent and return the response.\n",
    "\n",
    "    Args:\n",
    "        question: The user's question or message.\n",
    "        thread_id: Session ID for multi-turn conversation tracking.\n",
    "        verbose: If True, print step-by-step progress.\n",
    "\n",
    "    Returns:\n",
    "        The agent's final response string.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"\ud83e\uddd1 User: {question}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    # Only send what changes each turn. The MemorySaver checkpointer\n",
    "    # preserves everything else (retrieved_sources, user_role, etc.)\n",
    "    # across turns. We must NOT reset retrieved_sources here \u2014 the\n",
    "    # role_response flow skips retrieval and reuses prior sources.\n",
    "    input_state = {\n",
    "        \"current_query\": question,\n",
    "        \"messages\": [HumanMessage(content=question)],\n",
    "        \"retry_count\": 0,\n",
    "        \"role_ambiguous\": False,   # reset per turn; let the node set it\n",
    "    }\n",
    "\n",
    "    # Stream through the graph for visibility into each step\n",
    "    final_state = None\n",
    "    async for event in rag_app.astream(input_state, config=config):\n",
    "        for node_name, node_output in event.items():\n",
    "            if verbose:\n",
    "                print(f\"\\n  \u2500\u2500 Node: {node_name} \u2500\u2500\")\n",
    "            final_state = node_output\n",
    "\n",
    "    answer = final_state.get(\"final_answer\", \"I wasn't able to generate a response.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n{'\u2500'*70}\")\n",
    "        print(f\"\ud83e\udd16 Assistant:\\n{answer}\")\n",
    "        print(f\"{'\u2500'*70}\\n\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "print(\"ask() helper defined \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8 \u00b7 Example Conversations\n",
    "\n",
    "The following cells demonstrate each of the four key capabilities. **These will only produce real outputs when Certara.AI and Weaviate are running locally.** Otherwise, they serve as runnable templates showing the intended flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 \u2014 Capability 1: Clarity Check & Clarification Request\n",
    "\n",
    "The system should detect that the question is too vague and ask for specifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Vague question that should trigger clarification\n",
    "await ask(\n",
    "    \"What's the policy on that?\",\n",
    "    thread_id=\"demo-clarity\",\n",
    ")\n",
    "\n",
    "# Expected flow: classify_intent \u2192 \"needs_clarification\" \u2192 clarify \u2192 END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another ambiguous example\n",
    "await ask(\n",
    "    \"Tell me about the rules.\",\n",
    "    thread_id=\"demo-clarity-2\",\n",
    ")\n",
    "\n",
    "# Expected: System asks \"Which rules are you referring to? For example,\n",
    "# are you asking about travel policy, expense reimbursement, data privacy...?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 \u2014 Capability 2: Question Decomposition & Parallel Retrieval\n",
    "\n",
    "A complex question is split into focused sub-queries that each retrieve relevant chunks independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complex multi-part question\n",
    "await ask(\n",
    "    \"What is the maximum reimbursement for international travel, \"\n",
    "    \"and does the per-diem rate differ between Europe and Asia?\",\n",
    "    thread_id=\"demo-decompose\",\n",
    ")\n",
    "\n",
    "# Expected decomposition:\n",
    "#   1. \"maximum reimbursement international travel policy\"\n",
    "#   2. \"per-diem rate Europe travel policy\"\n",
    "#   3. \"per-diem rate Asia travel policy\"\n",
    "# All three queries run in parallel against Weaviate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 \u2014 Capability 3: Answer Evaluation & Retry\n",
    "\n",
    "If the initial retrieval doesn't yield a confident answer, the system automatically retries with refined queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Niche question that may require retry\n",
    "await ask(\n",
    "    \"Under what circumstances can an employee request an exception to the \"\n",
    "    \"standard equipment procurement policy for specialized research hardware?\",\n",
    "    thread_id=\"demo-retry\",\n",
    ")\n",
    "\n",
    "# Expected flow:\n",
    "#   classify \u2192 decompose \u2192 retrieve \u2192 generate \u2192 evaluate\n",
    "#   If confidence < 0.6: \u2192 retrieve (with new queries) \u2192 generate \u2192 evaluate\n",
    "#   Repeat up to MAX_RETRIEVAL_RETRIES times, then finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 \u2014 Capability 4: Multi-turn Follow-up Handling\n",
    "\n",
    "The system tracks conversation context across turns and decides when follow-up questions need new retrieval vs. when they can be answered from existing context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION = \"demo-followup\"\n",
    "\n",
    "# Turn 1: Initial question\n",
    "await ask(\n",
    "    \"What is our remote work policy?\",\n",
    "    thread_id=SESSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2: Follow-up that can be answered from existing context\n",
    "await ask(\n",
    "    \"Can you summarize that in bullet points?\",\n",
    "    thread_id=SESSION,\n",
    ")\n",
    "\n",
    "# Expected: intent=\"follow_up\", needs_retrieval=False\n",
    "# \u2192 skips retrieval, reformats existing answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 3: Follow-up that DOES need new retrieval\n",
    "await ask(\n",
    "    \"How does this differ from the policy for contractors?\",\n",
    "    thread_id=SESSION,\n",
    ")\n",
    "\n",
    "# Expected: intent=\"follow_up\", needs_retrieval=True\n",
    "# \u2192 decomposes into queries about contractor remote work policy\n",
    "# \u2192 retrieves new sources and generates comparative answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 4: Chitchat\n",
    "await ask(\n",
    "    \"Thanks, that's really helpful!\",\n",
    "    thread_id=SESSION,\n",
    ")\n",
    "\n",
    "# Expected: intent=\"chitchat\" \u2192 friendly response, no retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 \u2014 Capability 5: Role Disambiguation\n",
    "\n",
    "When policy documents contain different rules for different organizational roles, the system detects this after generation and asks the user to specify their role before finalizing the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 1: A question whose answer differs by role\n",
    "SESSION_ROLE = \"demo-role\"\n",
    "\n",
    "await ask(\n",
    "    \"What is the process for requesting time off?\",\n",
    "    thread_id=SESSION_ROLE,\n",
    ")\n",
    "\n",
    "# Expected flow:\n",
    "#   classify (clear_question) \u2192 decompose \u2192 retrieve \u2192 generate\n",
    "#   \u2192 check_role_ambiguity detects different processes for e.g.\n",
    "#     managers, ICs, contractors \u2192 ask_role_clarification \u2192 END\n",
    "#\n",
    "# Agent should respond with something like:\n",
    "#   \"I found information about requesting time off, but the process\n",
    "#    differs depending on your role:\n",
    "#    \u2022 Full-time employees: submit via HR portal\n",
    "#    \u2022 Contractors: coordinate with your project lead\n",
    "#    \u2022 Managers: use the management dashboard\n",
    "#    Which role applies to you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2: User provides their role\n",
    "await ask(\n",
    "    \"I'm a contractor.\",\n",
    "    thread_id=SESSION_ROLE,\n",
    ")\n",
    "\n",
    "# Expected flow:\n",
    "#   classify \u2192 intent=\"role_response\", user_role=\"contractor\"\n",
    "#   \u2192 generate_answer (with user_role set, reuses existing sources)\n",
    "#   \u2192 check_role_ambiguity (user_role known \u2192 skip)\n",
    "#   \u2192 evaluate \u2192 finalize\n",
    "#\n",
    "# Agent should now give a targeted answer for contractors only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 3: Follow-up \u2014 role context should persist\n",
    "await ask(\n",
    "    \"And how far in advance do I need to submit the request?\",\n",
    "    thread_id=SESSION_ROLE,\n",
    ")\n",
    "\n",
    "# Expected: intent=\"follow_up\", needs_retrieval=True\n",
    "# user_role=\"contractor\" persists from prior turn\n",
    "# \u2192 new retrieval scoped to contractor time-off lead time\n",
    "# \u2192 role check passes (role already known) \u2192 evaluate \u2192 finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9 \u00b7 Interactive Chat Loop\n",
    "\n",
    "Run this cell for an interactive session. Type `quit` or `exit` to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def interactive_chat():\n",
    "    \"\"\"Run an interactive chat loop with the RAG agent.\"\"\"\n",
    "    session_id = f\"interactive-{uuid.uuid4().hex[:8]}\"\n",
    "    print(\"\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\")\n",
    "    print(\"\u2551       Policy RAG Assistant \u2014 Interactive Mode                   \u2551\")\n",
    "    print(\"\u2551  Ask questions about company policies. Type 'quit' to exit.     \u2551\")\n",
    "    print(\"\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            break\n",
    "\n",
    "        if not user_input:\n",
    "            continue\n",
    "        if user_input.lower() in (\"quit\", \"exit\", \"q\"):\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "\n",
    "        await ask(user_input, thread_id=session_id)\n",
    "\n",
    "\n",
    "# Uncomment to run:\n",
    "# await interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10 \u00b7 Debugging & Inspection Utilities\n",
    "\n",
    "Helpers for inspecting graph state, tracing execution, and testing individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_thread_state(thread_id: str = \"default-session\"):\n",
    "    \"\"\"\n",
    "    Print the current state snapshot for a conversation thread.\n",
    "    Useful for debugging multi-turn interactions.\n",
    "    \"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    snapshot = rag_app.get_state(config)\n",
    "\n",
    "    if not snapshot or not snapshot.values:\n",
    "        print(f\"No state found for thread '{thread_id}'\")\n",
    "        return\n",
    "\n",
    "    state = snapshot.values\n",
    "    print(f\"\\n\ud83d\udccb State for thread '{thread_id}'\")\n",
    "    print(f\"   Messages:          {len(state.get('messages', []))}\")\n",
    "    print(f\"   Current query:     {state.get('current_query', 'N/A')}\")\n",
    "    print(f\"   Intent:            {state.get('intent', 'N/A')}\")\n",
    "    print(f\"   Sub-queries:       {state.get('sub_queries', [])}\")\n",
    "    print(f\"   Sources retrieved: {len(state.get('retrieved_sources', []))}\")\n",
    "    print(f\"   Confidence:        {state.get('confidence', 'N/A')}\")\n",
    "    print(f\"   Retry count:       {state.get('retry_count', 0)}\")\n",
    "\n",
    "    # Print message history\n",
    "    print(f\"\\n   Message History:\")\n",
    "    for i, msg in enumerate(state.get(\"messages\", [])):\n",
    "        role = \"\ud83e\uddd1 User\" if isinstance(msg, HumanMessage) else \"\ud83e\udd16 Asst\"\n",
    "        content_preview = msg.content[:100] + (\"...\" if len(msg.content) > 100 else \"\")\n",
    "        print(f\"   [{i}] {role}: {content_preview}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# inspect_thread_state(\"demo-followup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_retrieval(query: str):\n",
    "    \"\"\"\n",
    "    Test retrieval independently \u2014 useful for debugging search quality\n",
    "    without running the full pipeline.\n",
    "    \"\"\"\n",
    "    print(f\"Testing retrieval for: '{query}'\")\n",
    "    print(f\"{'\u2500'*50}\")\n",
    "\n",
    "    try:\n",
    "        results = await retrieve_from_weaviate(query)\n",
    "        for i, r in enumerate(results, 1):\n",
    "            print(f\"\\n  [{i}] Score: {r['score']:.4f}  |  Source: {r['source']}\")\n",
    "            print(f\"      {r['text'][:150]}...\")\n",
    "        print(f\"\\nTotal results: {len(results)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Retrieval failed: {e}\")\n",
    "\n",
    "\n",
    "# Example:\n",
    "# await test_retrieval(\"remote work eligibility requirements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_combined_rag(query: str):\n",
    "    \"\"\"\n",
    "    Test the combined RAG endpoint independently.\n",
    "    \"\"\"\n",
    "    print(f\"Testing combined RAG for: '{query}'\")\n",
    "    print(f\"{'\u2500'*50}\")\n",
    "\n",
    "    try:\n",
    "        result = await certara_rag_call(query)\n",
    "        print(f\"\\nAnswer:\\n{result.get('answer', 'N/A')}\")\n",
    "        print(f\"\\nSources:\")\n",
    "        for s in result.get(\"sources\", []):\n",
    "            print(f\"  \u2022 {s.get('source', 'unknown')} (score: {s.get('score', 'N/A')})\")\n",
    "    except Exception as e:\n",
    "        print(f\"RAG call failed: {e}\")\n",
    "\n",
    "\n",
    "# Example:\n",
    "# await test_combined_rag(\"What is the vacation accrual policy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11 \u00b7 Configuration Notes & Customization\n",
    "\n",
    "### Adapting to Your Environment\n",
    "\n",
    "| Setting | Where to Change | Notes |\n",
    "|---------|----------------|-------|\n",
    "| Certara.AI URL | `CERTARA_BASE_URL` in \u00a71 | Must expose OpenAI-compatible `/v1/chat/completions` |\n",
    "| Model name | `CERTARA_MODEL` in \u00a71 | Match your deployed model identifier |\n",
    "| Weaviate URL | `WEAVIATE_URL` in \u00a71 | Default Weaviate port is 8080 |\n",
    "| Collection name | `WEAVIATE_COLLECTION` in \u00a71 | Must match your ingested document collection |\n",
    "| Confidence threshold | `CONFIDENCE_THRESHOLD` in \u00a71 | Lower = more permissive, Higher = more retries |\n",
    "| Max retries | `MAX_RETRIEVAL_RETRIES` in \u00a71 | Cap on retrieval retry loops |\n",
    "| Chunks per query | `TOP_K_RESULTS` in \u00a71 | More chunks = more context but higher latency |\n",
    "\n",
    "### API Endpoint Assumptions\n",
    "\n",
    "This notebook assumes your Certara.AI deployment exposes three endpoints:\n",
    "\n",
    "```\n",
    "POST /v1/chat/completions    # Standard OpenAI-compatible generation\n",
    "POST /v1/retrieve            # Vector search against Weaviate\n",
    "POST /v1/rag                 # Combined retrieve + generate\n",
    "```\n",
    "\n",
    "If your API differs, update the `retrieve_from_weaviate()` and `certara_rag_call()` functions in \u00a72 to match your endpoint signatures and response formats.\n",
    "\n",
    "### Extending the Graph\n",
    "\n",
    "Common extensions you might add:\n",
    "\n",
    "- **Guardrails node** \u2014 check generated answers for policy compliance or PII before returning\n",
    "- **Routing to specialized sub-graphs** \u2014 e.g., different retrieval strategies for HR vs. Finance policies\n",
    "- **Human-in-the-loop** \u2014 use LangGraph's `interrupt_before` to pause before certain nodes and require approval\n",
    "- **Streaming** \u2014 replace `astream` with `astream_events` for token-level streaming to the UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
