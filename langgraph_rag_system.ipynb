{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG System with LangGraph + Certara.AI\n",
    "\n",
    "This notebook demonstrates a **Retrieval-Augmented Generation (RAG)** system built on top of **LangGraph** that queries a corpus of policy documents stored in a **Weaviate** vector database, with LLM inference powered by a local **Certara.AI** deployment.\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚                  LangGraph Agent                    â”‚\n",
    "                    â”‚                                                     â”‚\n",
    "  User â”€â”€â”€â”€â”€â”€â–º     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "  Question         â”‚  â”‚  Classify  â”‚â”€â”€â”€â–ºâ”‚  Decompose / â”‚â”€â”€â”€â–ºâ”‚ Parallel â”‚  â”‚\n",
    "                   â”‚  â”‚  Intent    â”‚    â”‚  Rephrase     â”‚    â”‚ Retrieve â”‚  â”‚\n",
    "                   â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚\n",
    "                   â”‚        â”‚ need info?                         â”‚        â”‚\n",
    "                   â”‚        â–¼                                    â–¼        â”‚\n",
    "                   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "                   â”‚  â”‚ Clarify / â”‚â—„â”€â”€â”€â”€ User             â”‚ Generate â”‚   â”‚\n",
    "                   â”‚  â”‚ Ask User  â”‚                       â”‚ Answer   â”‚   â”‚\n",
    "                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚\n",
    "                   â”‚                                            â”‚        â”‚\n",
    "                   â”‚                                      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”‚\n",
    "                   â”‚                                      â”‚ Evaluate â”‚   â”‚\n",
    "                   â”‚                                      â”‚ Answer   â”‚   â”‚\n",
    "                   â”‚                                      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚\n",
    "                   â”‚                                     pass / retry    â”‚\n",
    "                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                          â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚                                                       â”‚\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚  Certara.AI LLM â”‚  (local OpenAI-compatible API)    â”‚  Weaviate Vector â”‚\n",
    "     â”‚  /v1/chat/...   â”‚                                   â”‚  Database         â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Key Capabilities\n",
    "\n",
    "| # | Capability | Description |\n",
    "|---|-----------|-------------|\n",
    "| 1 | **Clarity Check** | Detects ambiguous or under-specified questions and asks the user for clarification |\n",
    "| 2 | **Question Decomposition** | Breaks complex questions into sub-queries and runs parallel retrievals |\n",
    "| 3 | **Answer Evaluation & Retry** | Scores the generated answer and retries retrieval if confidence is low |\n",
    "| 4 | **Conversational Follow-up** | Handles multi-turn dialogue, deciding when new retrieval is needed |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 Â· Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# %pip install langgraph langchain langchain-openai weaviate-client httpx pydantic --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import httpx\n",
    "import asyncio\n",
    "from typing import TypedDict, Annotated, Literal, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Certara.AI Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# These point to your local Certara.AI deployment which exposes an\n",
    "# OpenAI-compatible API for text generation.\n",
    "\n",
    "CERTARA_BASE_URL = os.getenv(\"CERTARA_BASE_URL\", \"http://localhost:8000/v1\")\n",
    "CERTARA_API_KEY  = os.getenv(\"CERTARA_API_KEY\", \"certara-local-key\")\n",
    "CERTARA_MODEL    = os.getenv(\"CERTARA_MODEL\", \"certara-default\")\n",
    "\n",
    "# â”€â”€ Weaviate Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# The vector DB that holds chunked policy documents.\n",
    "\n",
    "WEAVIATE_URL        = os.getenv(\"WEAVIATE_URL\", \"http://localhost:8080\")\n",
    "WEAVIATE_COLLECTION = os.getenv(\"WEAVIATE_COLLECTION\", \"PolicyDocuments\")\n",
    "\n",
    "# â”€â”€ Agent Parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MAX_RETRIEVAL_RETRIES = 2          # Max retry rounds for low-confidence answers\n",
    "CONFIDENCE_THRESHOLD  = 0.6        # Min confidence to accept an answer\n",
    "TOP_K_RESULTS         = 5          # Chunks to retrieve per sub-query\n",
    "\n",
    "print(f\"Certara.AI endpoint : {CERTARA_BASE_URL}\")\n",
    "print(f\"Weaviate endpoint   : {WEAVIATE_URL}\")\n",
    "print(f\"Collection          : {WEAVIATE_COLLECTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 Â· API Client Wrappers\n",
    "\n",
    "We wrap the three primary interaction modes with Certara.AI into clean helper functions:\n",
    "\n",
    "| Mode | Endpoint | Purpose |\n",
    "|------|----------|---------|\n",
    "| **Generate** | `POST /v1/chat/completions` | LLM text generation |\n",
    "| **Retrieve** | `POST /v1/retrieve` | Semantic search over Weaviate |\n",
    "| **RAG (combined)** | `POST /v1/rag` | Retrieve + Generate in one call |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ LLM via LangChain (uses the OpenAI-compatible /v1/chat/completions) â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=CERTARA_BASE_URL,\n",
    "    api_key=CERTARA_API_KEY,\n",
    "    model=CERTARA_MODEL,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Quick smoke test (will fail gracefully if Certara.AI is not running)\n",
    "try:\n",
    "    resp = llm.invoke([HumanMessage(content=\"Say hello in one sentence.\")])\n",
    "    print(f\"LLM connected âœ“  Response: {resp.content[:80]}\")\n",
    "except Exception as e:\n",
    "    print(f\"LLM not reachable (expected if Certara.AI is not running): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Weaviate Retrieval Client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "async def retrieve_from_weaviate(\n",
    "    query: str,\n",
    "    top_k: int = TOP_K_RESULTS,\n",
    "    collection: str = WEAVIATE_COLLECTION,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Mode 2: Retrieve relevant chunks from the Weaviate vector database\n",
    "    via the Certara.AI retrieval endpoint.\n",
    "\n",
    "    Returns a list of dicts with keys: text, source, score, metadata.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"collection\": collection,\n",
    "        \"top_k\": top_k,\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=30) as client:\n",
    "        response = await client.post(\n",
    "            f\"{CERTARA_BASE_URL.rstrip('/v1')}/v1/retrieve\",\n",
    "            json=payload,\n",
    "            headers={\"Authorization\": f\"Bearer {CERTARA_API_KEY}\"},\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "    # Normalize the response into a consistent format\n",
    "    results = []\n",
    "    for item in data.get(\"results\", []):\n",
    "        results.append({\n",
    "            \"text\": item.get(\"text\", item.get(\"content\", \"\")),\n",
    "            \"source\": item.get(\"source\", item.get(\"metadata\", {}).get(\"source\", \"unknown\")),\n",
    "            \"score\": item.get(\"score\", item.get(\"distance\", 0.0)),\n",
    "            \"metadata\": item.get(\"metadata\", {}),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "async def parallel_retrieve(queries: list[str], top_k: int = TOP_K_RESULTS) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Run multiple retrieval queries in parallel and deduplicate results.\n",
    "    \"\"\"\n",
    "    tasks = [retrieve_from_weaviate(q, top_k=top_k) for q in queries]\n",
    "    all_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    # Flatten and deduplicate by text content\n",
    "    seen_texts = set()\n",
    "    deduplicated = []\n",
    "    for result_set in all_results:\n",
    "        if isinstance(result_set, Exception):\n",
    "            print(f\"  âš  Retrieval error: {result_set}\")\n",
    "            continue\n",
    "        for r in result_set:\n",
    "            text_key = r[\"text\"][:200]  # dedup on first 200 chars\n",
    "            if text_key not in seen_texts:\n",
    "                seen_texts.add(text_key)\n",
    "                deduplicated.append(r)\n",
    "\n",
    "    # Sort by relevance score (higher is better)\n",
    "    deduplicated.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return deduplicated\n",
    "\n",
    "\n",
    "print(\"Retrieval functions defined âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Combined RAG Client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "async def certara_rag_call(\n",
    "    query: str,\n",
    "    top_k: int = TOP_K_RESULTS,\n",
    "    system_prompt: str = \"You are a helpful policy assistant.\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Mode 3: Combined Retrieve + Generate call to Certara.AI.\n",
    "    The server handles both retrieval and generation in a single request.\n",
    "\n",
    "    Returns: {\"answer\": str, \"sources\": list[dict]}\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"collection\": WEAVIATE_COLLECTION,\n",
    "        \"top_k\": top_k,\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"model\": CERTARA_MODEL,\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=60) as client:\n",
    "        response = await client.post(\n",
    "            f\"{CERTARA_BASE_URL.rstrip('/v1')}/v1/rag\",\n",
    "            json=payload,\n",
    "            headers={\"Authorization\": f\"Bearer {CERTARA_API_KEY}\"},\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "print(\"RAG client defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 Â· Graph State Definition\n",
    "\n",
    "LangGraph uses a typed state object that flows through every node in the graph. Our state tracks the full conversation history, retrieved sources, decomposed sub-queries, confidence scores, and retry counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(TypedDict):\n",
    "    \"\"\"State that flows through the LangGraph RAG pipeline.\"\"\"\n",
    "\n",
    "    # â”€â”€ Conversation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    messages: Annotated[list[BaseMessage], add_messages]  # full chat history\n",
    "    current_query: str                                     # latest user question\n",
    "\n",
    "    # â”€â”€ Intent Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    intent: str              # \"clear_question\" | \"needs_clarification\" | \"follow_up\" | \"chitchat\"\n",
    "    needs_retrieval: bool    # whether follow-up requires new retrieval\n",
    "\n",
    "    # â”€â”€ Decomposition & Retrieval â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    sub_queries: list[str]           # decomposed / rephrased queries\n",
    "    retrieved_sources: list[dict]    # chunks from Weaviate\n",
    "\n",
    "    # â”€â”€ Generation & Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    draft_answer: str        # generated answer (before evaluation)\n",
    "    confidence: float        # 0.0 â€“ 1.0 self-assessed confidence\n",
    "    retry_count: int         # number of retrieval retries so far\n",
    "    final_answer: str        # accepted answer to present to user\n",
    "\n",
    "\n",
    "print(\"State schema defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 Â· Graph Nodes\n",
    "\n",
    "Each node is an async function that reads state, performs work, and returns a partial state update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 â€” Classify Intent\n",
    "\n",
    "The first node examines the user's message in context of the conversation history and classifies it as:\n",
    "- **`clear_question`** â€” well-formed policy question â†’ proceed to decomposition  \n",
    "- **`needs_clarification`** â€” ambiguous or missing information â†’ ask user  \n",
    "- **`follow_up`** â€” continuation of a prior topic â†’ check if new retrieval is needed  \n",
    "- **`chitchat`** â€” greeting or off-topic â†’ respond directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFY_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an intent classifier for a policy Q&A system.\n",
    "\n",
    "Given the conversation history and the latest user message, classify the intent\n",
    "into EXACTLY ONE of the following categories and respond with valid JSON only:\n",
    "\n",
    "{{\n",
    "  \"intent\": \"clear_question\" | \"needs_clarification\" | \"follow_up\" | \"chitchat\",\n",
    "  \"needs_retrieval\": true | false,\n",
    "  \"reasoning\": \"<one sentence explaining your classification>\"\n",
    "}}\n",
    "\n",
    "Guidelines:\n",
    "- \"clear_question\": The user asks a specific, answerable question about policy.\n",
    "- \"needs_clarification\": The question is too vague, ambiguous, or references\n",
    "  something unspecified (e.g., \"What's the policy?\" without saying WHICH policy,\n",
    "  or \"Tell me about the rules\" without context).\n",
    "- \"follow_up\": The user is continuing a previous topic â€” asking for more detail,\n",
    "  rephrasing, or adding a related question. Set needs_retrieval=true if answering\n",
    "  likely requires new information beyond what was already retrieved.\n",
    "- \"chitchat\": Greetings, thanks, or off-topic messages.\n",
    "\n",
    "Respond ONLY with the JSON object, no other text.\"\"\"),\n",
    "    (\"human\", \"\"\"Conversation so far:\n",
    "{history}\n",
    "\n",
    "Latest user message: {query}\"\"\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def classify_intent(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Classify the user's intent.\"\"\"\n",
    "    # Build a readable history string\n",
    "    history_lines = []\n",
    "    for msg in state.get(\"messages\", [])[:-1]:  # exclude latest\n",
    "        role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "        history_lines.append(f\"{role}: {msg.content}\")\n",
    "    history_str = \"\\n\".join(history_lines[-10:]) or \"(no prior conversation)\"\n",
    "\n",
    "    chain = CLASSIFY_PROMPT | llm | StrOutputParser()\n",
    "    raw = await chain.ainvoke({\"history\": history_str, \"query\": state[\"current_query\"]})\n",
    "\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback if the LLM didn't return clean JSON\n",
    "        parsed = {\"intent\": \"clear_question\", \"needs_retrieval\": True, \"reasoning\": \"Parse error, defaulting.\"}\n",
    "\n",
    "    print(f\"  ğŸ·  Intent: {parsed['intent']}  |  Needs retrieval: {parsed.get('needs_retrieval', True)}\")\n",
    "    print(f\"     Reasoning: {parsed.get('reasoning', '')}\")\n",
    "\n",
    "    return {\n",
    "        \"intent\": parsed[\"intent\"],\n",
    "        \"needs_retrieval\": parsed.get(\"needs_retrieval\", True),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"classify_intent node defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 â€” Clarify (Ask User for More Information)\n",
    "\n",
    "When the question is too vague, this node generates a clarification request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLARIFY_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful policy assistant. The user's question is\n",
    "ambiguous or lacks sufficient detail for you to look up an answer in the policy\n",
    "database. Write a concise, friendly message asking the user to clarify.\n",
    "Be specific about WHAT information is missing.\"\"\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def clarify(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Ask the user for clarification.\"\"\"\n",
    "    chain = CLARIFY_PROMPT | llm | StrOutputParser()\n",
    "    clarification = await chain.ainvoke({\"query\": state[\"current_query\"]})\n",
    "\n",
    "    print(f\"  â“ Asking for clarification\")\n",
    "\n",
    "    return {\n",
    "        \"final_answer\": clarification,\n",
    "        \"messages\": [AIMessage(content=clarification)],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"clarify node defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 â€” Decompose & Rephrase Queries\n",
    "\n",
    "Complex questions are broken into focused sub-queries that are individually optimized for vector-similarity retrieval. This improves recall significantly over sending a single long question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECOMPOSE_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a query decomposition engine for a policy document search system.\n",
    "\n",
    "Given the user's question (and conversation history for context), produce 1â€“4\n",
    "focused search queries that together cover the full information need.\n",
    "\n",
    "Rules:\n",
    "- Each sub-query should be a short, specific phrase optimized for semantic search.\n",
    "- Replace pronouns with their referents using conversation context.\n",
    "- If the question is already simple and specific, return it as a single query.\n",
    "- Do NOT add queries for information the user did not ask about.\n",
    "\n",
    "Respond with valid JSON only:\n",
    "{{\"queries\": [\"query 1\", \"query 2\", ...]}}\"\"\"),\n",
    "    (\"human\", \"\"\"Conversation context:\n",
    "{history}\n",
    "\n",
    "Current question: {query}\"\"\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def decompose_query(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Decompose / rephrase the question into retrieval-optimized sub-queries.\"\"\"\n",
    "    history_lines = []\n",
    "    for msg in state.get(\"messages\", [])[:-1]:\n",
    "        role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "        history_lines.append(f\"{role}: {msg.content}\")\n",
    "    history_str = \"\\n\".join(history_lines[-6:]) or \"(none)\"\n",
    "\n",
    "    chain = DECOMPOSE_PROMPT | llm | StrOutputParser()\n",
    "    raw = await chain.ainvoke({\"history\": history_str, \"query\": state[\"current_query\"]})\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        sub_queries = parsed.get(\"queries\", [state[\"current_query\"]])\n",
    "    except json.JSONDecodeError:\n",
    "        sub_queries = [state[\"current_query\"]]\n",
    "\n",
    "    print(f\"  ğŸ”€ Decomposed into {len(sub_queries)} sub-queries:\")\n",
    "    for i, q in enumerate(sub_queries, 1):\n",
    "        print(f\"     {i}. {q}\")\n",
    "\n",
    "    return {\"sub_queries\": sub_queries}\n",
    "\n",
    "\n",
    "print(\"decompose_query node defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 â€” Parallel Retrieval\n",
    "\n",
    "All sub-queries are dispatched to Weaviate **in parallel** via `asyncio.gather`. Results are deduplicated and sorted by relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Run parallel retrieval for all sub-queries against Weaviate.\"\"\"\n",
    "    queries = state.get(\"sub_queries\", [state[\"current_query\"]])\n",
    "\n",
    "    print(f\"  ğŸ” Retrieving from Weaviate ({len(queries)} parallel queries)...\")\n",
    "    sources = await parallel_retrieve(queries, top_k=TOP_K_RESULTS)\n",
    "\n",
    "    # Merge with any previously retrieved sources (for retries)\n",
    "    existing = state.get(\"retrieved_sources\", [])\n",
    "    existing_texts = {s[\"text\"][:200] for s in existing}\n",
    "    for s in sources:\n",
    "        if s[\"text\"][:200] not in existing_texts:\n",
    "            existing.append(s)\n",
    "\n",
    "    print(f\"  ğŸ“„ Retrieved {len(sources)} new chunks ({len(existing)} total)\")\n",
    "\n",
    "    return {\"retrieved_sources\": existing}\n",
    "\n",
    "\n",
    "print(\"retrieve node defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 â€” Generate Answer\n",
    "\n",
    "The generation node assembles retrieved context into a prompt and calls the LLM to produce a grounded answer with citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a policy expert assistant. Answer the user's question\n",
    "using ONLY the provided source documents. Follow these rules:\n",
    "\n",
    "1. Base your answer strictly on the retrieved sources.\n",
    "2. Cite sources using [Source: <filename>] notation.\n",
    "3. If the sources do not contain enough information, say so clearly.\n",
    "4. Be concise but thorough.\n",
    "5. If multiple sources conflict, note the discrepancy.\n",
    "\n",
    "Retrieved Sources:\n",
    "{context}\"\"\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def generate_answer(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Generate an answer grounded in retrieved sources.\"\"\"\n",
    "    sources = state.get(\"retrieved_sources\", [])\n",
    "\n",
    "    # Format context block\n",
    "    context_parts = []\n",
    "    for i, s in enumerate(sources[:10], 1):  # Limit to top 10\n",
    "        context_parts.append(\n",
    "            f\"--- Source {i} [Source: {s['source']}] (score: {s['score']:.3f}) ---\\n{s['text']}\"\n",
    "        )\n",
    "    context_str = \"\\n\\n\".join(context_parts) or \"(No sources retrieved)\"\n",
    "\n",
    "    chain = GENERATE_PROMPT | llm | StrOutputParser()\n",
    "    answer = await chain.ainvoke({\"context\": context_str, \"query\": state[\"current_query\"]})\n",
    "\n",
    "    print(f\"  âœ  Draft answer generated ({len(answer)} chars)\")\n",
    "\n",
    "    return {\"draft_answer\": answer}\n",
    "\n",
    "\n",
    "print(\"generate_answer node defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 â€” Evaluate Answer\n",
    "\n",
    "The evaluation node uses a separate LLM call to score the draft answer for:\n",
    "- **Relevance**: Does it address what was asked?\n",
    "- **Groundedness**: Is it supported by the retrieved sources?\n",
    "- **Completeness**: Does it fully answer the question?\n",
    "\n",
    "If confidence is below the threshold, the system retries with refined queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATE_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a critical evaluator for a RAG system. Assess the quality\n",
    "of the generated answer given the original question and the retrieved sources.\n",
    "\n",
    "Score each dimension from 0.0 to 1.0:\n",
    "- relevance: Does the answer address the question?\n",
    "- groundedness: Is every claim supported by the sources?\n",
    "- completeness: Does the answer fully address all parts of the question?\n",
    "\n",
    "Also suggest what additional information to search for if the answer is incomplete.\n",
    "\n",
    "Respond with valid JSON only:\n",
    "{{\n",
    "  \"relevance\": 0.0-1.0,\n",
    "  \"groundedness\": 0.0-1.0,\n",
    "  \"completeness\": 0.0-1.0,\n",
    "  \"overall_confidence\": 0.0-1.0,\n",
    "  \"missing_info\": \"description of what's missing, or 'none'\",\n",
    "  \"retry_queries\": [\"suggested search query 1\", ...]\n",
    "}}\"\"\"),\n",
    "    (\"human\", \"\"\"Question: {query}\n",
    "\n",
    "Generated Answer:\n",
    "{answer}\n",
    "\n",
    "Sources used (count): {source_count}\"\"\"),\n",
    "])\n",
    "\n",
    "\n",
    "async def evaluate_answer(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Evaluate the draft answer and decide whether to accept or retry.\"\"\"\n",
    "    chain = EVALUATE_PROMPT | llm | StrOutputParser()\n",
    "    raw = await chain.ainvoke({\n",
    "        \"query\": state[\"current_query\"],\n",
    "        \"answer\": state[\"draft_answer\"],\n",
    "        \"source_count\": len(state.get(\"retrieved_sources\", [])),\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        evaluation = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        evaluation = {\"overall_confidence\": 0.7, \"retry_queries\": []}\n",
    "\n",
    "    confidence = evaluation.get(\"overall_confidence\", 0.7)\n",
    "    retry_queries = evaluation.get(\"retry_queries\", [])\n",
    "\n",
    "    print(f\"  ğŸ“Š Evaluation â€” Confidence: {confidence:.2f}\")\n",
    "    print(f\"     Relevance:    {evaluation.get('relevance', 'N/A')}\")\n",
    "    print(f\"     Groundedness: {evaluation.get('groundedness', 'N/A')}\")\n",
    "    print(f\"     Completeness: {evaluation.get('completeness', 'N/A')}\")\n",
    "    if evaluation.get(\"missing_info\", \"none\") != \"none\":\n",
    "        print(f\"     Missing info: {evaluation['missing_info']}\")\n",
    "\n",
    "    return {\n",
    "        \"confidence\": confidence,\n",
    "        \"sub_queries\": retry_queries if confidence < CONFIDENCE_THRESHOLD else [],\n",
    "        \"retry_count\": state.get(\"retry_count\", 0) + (1 if confidence < CONFIDENCE_THRESHOLD else 0),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"evaluate_answer node defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 â€” Finalize Answer\n",
    "\n",
    "Once the answer passes evaluation (or max retries are exhausted), this node packages the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def finalize_answer(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Accept the draft answer as final and add it to the message history.\"\"\"\n",
    "    answer = state[\"draft_answer\"]\n",
    "\n",
    "    # If confidence was low and retries exhausted, prepend a caveat\n",
    "    if state.get(\"confidence\", 1.0) < CONFIDENCE_THRESHOLD:\n",
    "        caveat = (\n",
    "            \"âš ï¸ **Note:** I was unable to find highly relevant sources for all \"\n",
    "            \"parts of your question. The answer below is based on the best \"\n",
    "            \"available information, but may be incomplete.\\n\\n\"\n",
    "        )\n",
    "        answer = caveat + answer\n",
    "\n",
    "    # Append source citations summary\n",
    "    sources = state.get(\"retrieved_sources\", [])\n",
    "    if sources:\n",
    "        unique_sources = sorted(set(s[\"source\"] for s in sources))\n",
    "        source_list = \"\\n\".join(f\"  â€¢ {s}\" for s in unique_sources)\n",
    "        answer += f\"\\n\\n---\\n**Sources consulted:**\\n{source_list}\"\n",
    "\n",
    "    print(f\"  âœ… Answer finalized\")\n",
    "\n",
    "    return {\n",
    "        \"final_answer\": answer,\n",
    "        \"messages\": [AIMessage(content=answer)],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"finalize_answer node defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 â€” Handle Chitchat\n",
    "\n",
    "Simple conversational responses that don't require retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_chitchat(state: RAGState) -> dict:\n",
    "    \"\"\"Node: Respond to non-policy chitchat (greetings, thanks, etc.).\"\"\"\n",
    "    response = await llm.ainvoke([\n",
    "        SystemMessage(content=(\n",
    "            \"You are a friendly policy assistant. The user sent a casual message \"\n",
    "            \"(greeting, thanks, etc.). Respond briefly and warmly, and let them \"\n",
    "            \"know you're here to help with policy questions.\"\n",
    "        )),\n",
    "        HumanMessage(content=state[\"current_query\"]),\n",
    "    ])\n",
    "\n",
    "    print(f\"  ğŸ’¬ Chitchat response\")\n",
    "\n",
    "    return {\n",
    "        \"final_answer\": response.content,\n",
    "        \"messages\": [AIMessage(content=response.content)],\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"handle_chitchat node defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 Â· Routing Logic (Conditional Edges)\n",
    "\n",
    "LangGraph uses conditional edges to decide which node to visit next. These functions implement our routing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_classification(state: RAGState) -> str:\n",
    "    \"\"\"\n",
    "    After classifying intent, decide the next step:\n",
    "      - needs_clarification â†’ ask user\n",
    "      - chitchat â†’ respond directly\n",
    "      - follow_up (no retrieval needed) â†’ generate from existing sources\n",
    "      - clear_question / follow_up (retrieval needed) â†’ decompose & retrieve\n",
    "    \"\"\"\n",
    "    intent = state.get(\"intent\", \"clear_question\")\n",
    "\n",
    "    if intent == \"needs_clarification\":\n",
    "        return \"clarify\"\n",
    "    elif intent == \"chitchat\":\n",
    "        return \"handle_chitchat\"\n",
    "    elif intent == \"follow_up\" and not state.get(\"needs_retrieval\", True):\n",
    "        # Follow-up that can be answered from existing context\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        # clear_question or follow_up needing retrieval\n",
    "        return \"decompose_query\"\n",
    "\n",
    "\n",
    "def route_after_evaluation(state: RAGState) -> str:\n",
    "    \"\"\"\n",
    "    After evaluating the answer, decide whether to:\n",
    "      - Accept the answer (confidence >= threshold or retries exhausted)\n",
    "      - Retry with new queries (confidence < threshold and retries remaining)\n",
    "    \"\"\"\n",
    "    confidence = state.get(\"confidence\", 1.0)\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "\n",
    "    if confidence >= CONFIDENCE_THRESHOLD:\n",
    "        print(f\"  âœ… Confidence {confidence:.2f} >= {CONFIDENCE_THRESHOLD} â†’ accepting\")\n",
    "        return \"finalize_answer\"\n",
    "    elif retry_count >= MAX_RETRIEVAL_RETRIES:\n",
    "        print(f\"  âš ï¸  Max retries ({MAX_RETRIEVAL_RETRIES}) reached â†’ accepting with caveat\")\n",
    "        return \"finalize_answer\"\n",
    "    else:\n",
    "        print(f\"  ğŸ”„ Confidence {confidence:.2f} < {CONFIDENCE_THRESHOLD} â†’ retrying (attempt {retry_count})\")\n",
    "        return \"retrieve\"  # retry with the new sub_queries set by evaluate\n",
    "\n",
    "\n",
    "print(\"Routing functions defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 Â· Build the LangGraph\n",
    "\n",
    "Now we assemble all nodes and edges into the complete graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_graph() -> StateGraph:\n",
    "    \"\"\"Construct the full RAG agent graph.\"\"\"\n",
    "\n",
    "    graph = StateGraph(RAGState)\n",
    "\n",
    "    # â”€â”€ Add nodes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    graph.add_node(\"classify_intent\",  classify_intent)\n",
    "    graph.add_node(\"clarify\",          clarify)\n",
    "    graph.add_node(\"handle_chitchat\",  handle_chitchat)\n",
    "    graph.add_node(\"decompose_query\",  decompose_query)\n",
    "    graph.add_node(\"retrieve\",         retrieve)\n",
    "    graph.add_node(\"generate_answer\",  generate_answer)\n",
    "    graph.add_node(\"evaluate_answer\",  evaluate_answer)\n",
    "    graph.add_node(\"finalize_answer\",  finalize_answer)\n",
    "\n",
    "    # â”€â”€ Entry point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    graph.set_entry_point(\"classify_intent\")\n",
    "\n",
    "    # â”€â”€ Conditional: after classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    graph.add_conditional_edges(\n",
    "        \"classify_intent\",\n",
    "        route_after_classification,\n",
    "        {\n",
    "            \"clarify\":          \"clarify\",\n",
    "            \"handle_chitchat\":  \"handle_chitchat\",\n",
    "            \"decompose_query\":  \"decompose_query\",\n",
    "            \"generate_answer\":  \"generate_answer\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Linear edges â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    graph.add_edge(\"decompose_query\", \"retrieve\")\n",
    "    graph.add_edge(\"retrieve\",        \"generate_answer\")\n",
    "    graph.add_edge(\"generate_answer\", \"evaluate_answer\")\n",
    "\n",
    "    # â”€â”€ Conditional: after evaluation (accept vs retry) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    graph.add_conditional_edges(\n",
    "        \"evaluate_answer\",\n",
    "        route_after_evaluation,\n",
    "        {\n",
    "            \"finalize_answer\": \"finalize_answer\",\n",
    "            \"retrieve\":        \"retrieve\",   # retry loop\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Terminal edges â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    graph.add_edge(\"clarify\",         END)\n",
    "    graph.add_edge(\"handle_chitchat\", END)\n",
    "    graph.add_edge(\"finalize_answer\", END)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "# Build and compile with memory for multi-turn conversations\n",
    "memory = MemorySaver()\n",
    "rag_graph = build_rag_graph()\n",
    "rag_app = rag_graph.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Graph compiled âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 â€” Visualize the Graph\n",
    "\n",
    "LangGraph can render a visual representation of the state machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph structure (requires graphviz or mermaid support)\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    img_bytes = rag_app.get_graph().draw_mermaid_png()\n",
    "    display(Image(img_bytes))\n",
    "except Exception as e:\n",
    "    # Fallback: print the mermaid diagram as text\n",
    "    print(\"Graph visualization (Mermaid format):\")\n",
    "    print(rag_app.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 Â· Conversation Runner\n",
    "\n",
    "A helper function that manages the interaction loop â€” feeding user messages into the graph, displaying results, and maintaining session state across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask(\n",
    "    question: str,\n",
    "    thread_id: str = \"default-session\",\n",
    "    verbose: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Send a question to the RAG agent and return the response.\n",
    "\n",
    "    Args:\n",
    "        question: The user's question or message.\n",
    "        thread_id: Session ID for multi-turn conversation tracking.\n",
    "        verbose: If True, print step-by-step progress.\n",
    "\n",
    "    Returns:\n",
    "        The agent's final response string.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ§‘ User: {question}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    input_state = {\n",
    "        \"current_query\": question,\n",
    "        \"messages\": [HumanMessage(content=question)],\n",
    "        \"retry_count\": 0,\n",
    "        \"retrieved_sources\": [],\n",
    "    }\n",
    "\n",
    "    # Stream through the graph for visibility into each step\n",
    "    final_state = None\n",
    "    async for event in rag_app.astream(input_state, config=config):\n",
    "        for node_name, node_output in event.items():\n",
    "            if verbose:\n",
    "                print(f\"\\n  â”€â”€ Node: {node_name} â”€â”€\")\n",
    "            final_state = node_output\n",
    "\n",
    "    answer = final_state.get(\"final_answer\", \"I wasn't able to generate a response.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(f\"ğŸ¤– Assistant:\\n{answer}\")\n",
    "        print(f\"{'â”€'*70}\\n\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "print(\"ask() helper defined âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8 Â· Example Conversations\n",
    "\n",
    "The following cells demonstrate each of the four key capabilities. **These will only produce real outputs when Certara.AI and Weaviate are running locally.** Otherwise, they serve as runnable templates showing the intended flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 â€” Capability 1: Clarity Check & Clarification Request\n",
    "\n",
    "The system should detect that the question is too vague and ask for specifics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Vague question that should trigger clarification\n",
    "await ask(\n",
    "    \"What's the policy on that?\",\n",
    "    thread_id=\"demo-clarity\",\n",
    ")\n",
    "\n",
    "# Expected flow: classify_intent â†’ \"needs_clarification\" â†’ clarify â†’ END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another ambiguous example\n",
    "await ask(\n",
    "    \"Tell me about the rules.\",\n",
    "    thread_id=\"demo-clarity-2\",\n",
    ")\n",
    "\n",
    "# Expected: System asks \"Which rules are you referring to? For example,\n",
    "# are you asking about travel policy, expense reimbursement, data privacy...?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 â€” Capability 2: Question Decomposition & Parallel Retrieval\n",
    "\n",
    "A complex question is split into focused sub-queries that each retrieve relevant chunks independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complex multi-part question\n",
    "await ask(\n",
    "    \"What is the maximum reimbursement for international travel, \"\n",
    "    \"and does the per-diem rate differ between Europe and Asia?\",\n",
    "    thread_id=\"demo-decompose\",\n",
    ")\n",
    "\n",
    "# Expected decomposition:\n",
    "#   1. \"maximum reimbursement international travel policy\"\n",
    "#   2. \"per-diem rate Europe travel policy\"\n",
    "#   3. \"per-diem rate Asia travel policy\"\n",
    "# All three queries run in parallel against Weaviate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 â€” Capability 3: Answer Evaluation & Retry\n",
    "\n",
    "If the initial retrieval doesn't yield a confident answer, the system automatically retries with refined queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Niche question that may require retry\n",
    "await ask(\n",
    "    \"Under what circumstances can an employee request an exception to the \"\n",
    "    \"standard equipment procurement policy for specialized research hardware?\",\n",
    "    thread_id=\"demo-retry\",\n",
    ")\n",
    "\n",
    "# Expected flow:\n",
    "#   classify â†’ decompose â†’ retrieve â†’ generate â†’ evaluate\n",
    "#   If confidence < 0.6: â†’ retrieve (with new queries) â†’ generate â†’ evaluate\n",
    "#   Repeat up to MAX_RETRIEVAL_RETRIES times, then finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 â€” Capability 4: Multi-turn Follow-up Handling\n",
    "\n",
    "The system tracks conversation context across turns and decides when follow-up questions need new retrieval vs. when they can be answered from existing context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION = \"demo-followup\"\n",
    "\n",
    "# Turn 1: Initial question\n",
    "await ask(\n",
    "    \"What is our remote work policy?\",\n",
    "    thread_id=SESSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2: Follow-up that can be answered from existing context\n",
    "await ask(\n",
    "    \"Can you summarize that in bullet points?\",\n",
    "    thread_id=SESSION,\n",
    ")\n",
    "\n",
    "# Expected: intent=\"follow_up\", needs_retrieval=False\n",
    "# â†’ skips retrieval, reformats existing answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 3: Follow-up that DOES need new retrieval\n",
    "await ask(\n",
    "    \"How does this differ from the policy for contractors?\",\n",
    "    thread_id=SESSION,\n",
    ")\n",
    "\n",
    "# Expected: intent=\"follow_up\", needs_retrieval=True\n",
    "# â†’ decomposes into queries about contractor remote work policy\n",
    "# â†’ retrieves new sources and generates comparative answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 4: Chitchat\n",
    "await ask(\n",
    "    \"Thanks, that's really helpful!\",\n",
    "    thread_id=SESSION,\n",
    ")\n",
    "\n",
    "# Expected: intent=\"chitchat\" â†’ friendly response, no retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9 Â· Interactive Chat Loop\n",
    "\n",
    "Run this cell for an interactive session. Type `quit` or `exit` to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def interactive_chat():\n",
    "    \"\"\"Run an interactive chat loop with the RAG agent.\"\"\"\n",
    "    session_id = \"interactive-session\"\n",
    "    print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "    print(\"â•‘       Policy RAG Assistant â€” Interactive Mode                   â•‘\")\n",
    "    print(\"â•‘  Ask questions about company policies. Type 'quit' to exit.     â•‘\")\n",
    "    print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\\n\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            break\n",
    "\n",
    "        if not user_input:\n",
    "            continue\n",
    "        if user_input.lower() in (\"quit\", \"exit\", \"q\"):\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "\n",
    "        await ask(user_input, thread_id=session_id)\n",
    "\n",
    "\n",
    "# Uncomment to run:\n",
    "# await interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10 Â· Debugging & Inspection Utilities\n",
    "\n",
    "Helpers for inspecting graph state, tracing execution, and testing individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_thread_state(thread_id: str = \"default-session\"):\n",
    "    \"\"\"\n",
    "    Print the current state snapshot for a conversation thread.\n",
    "    Useful for debugging multi-turn interactions.\n",
    "    \"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    snapshot = rag_app.get_state(config)\n",
    "\n",
    "    if not snapshot or not snapshot.values:\n",
    "        print(f\"No state found for thread '{thread_id}'\")\n",
    "        return\n",
    "\n",
    "    state = snapshot.values\n",
    "    print(f\"\\nğŸ“‹ State for thread '{thread_id}'\")\n",
    "    print(f\"   Messages:          {len(state.get('messages', []))}\")\n",
    "    print(f\"   Current query:     {state.get('current_query', 'N/A')}\")\n",
    "    print(f\"   Intent:            {state.get('intent', 'N/A')}\")\n",
    "    print(f\"   Sub-queries:       {state.get('sub_queries', [])}\")\n",
    "    print(f\"   Sources retrieved: {len(state.get('retrieved_sources', []))}\")\n",
    "    print(f\"   Confidence:        {state.get('confidence', 'N/A')}\")\n",
    "    print(f\"   Retry count:       {state.get('retry_count', 0)}\")\n",
    "\n",
    "    # Print message history\n",
    "    print(f\"\\n   Message History:\")\n",
    "    for i, msg in enumerate(state.get(\"messages\", [])):\n",
    "        role = \"ğŸ§‘ User\" if isinstance(msg, HumanMessage) else \"ğŸ¤– Asst\"\n",
    "        content_preview = msg.content[:100] + (\"...\" if len(msg.content) > 100 else \"\")\n",
    "        print(f\"   [{i}] {role}: {content_preview}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# inspect_thread_state(\"demo-followup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_retrieval(query: str):\n",
    "    \"\"\"\n",
    "    Test retrieval independently â€” useful for debugging search quality\n",
    "    without running the full pipeline.\n",
    "    \"\"\"\n",
    "    print(f\"Testing retrieval for: '{query}'\")\n",
    "    print(f\"{'â”€'*50}\")\n",
    "\n",
    "    try:\n",
    "        results = await retrieve_from_weaviate(query)\n",
    "        for i, r in enumerate(results, 1):\n",
    "            print(f\"\\n  [{i}] Score: {r['score']:.4f}  |  Source: {r['source']}\")\n",
    "            print(f\"      {r['text'][:150]}...\")\n",
    "        print(f\"\\nTotal results: {len(results)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Retrieval failed: {e}\")\n",
    "\n",
    "\n",
    "# Example:\n",
    "# await test_retrieval(\"remote work eligibility requirements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_combined_rag(query: str):\n",
    "    \"\"\"\n",
    "    Test the combined RAG endpoint independently.\n",
    "    \"\"\"\n",
    "    print(f\"Testing combined RAG for: '{query}'\")\n",
    "    print(f\"{'â”€'*50}\")\n",
    "\n",
    "    try:\n",
    "        result = await certara_rag_call(query)\n",
    "        print(f\"\\nAnswer:\\n{result.get('answer', 'N/A')}\")\n",
    "        print(f\"\\nSources:\")\n",
    "        for s in result.get(\"sources\", []):\n",
    "            print(f\"  â€¢ {s.get('source', 'unknown')} (score: {s.get('score', 'N/A')})\")\n",
    "    except Exception as e:\n",
    "        print(f\"RAG call failed: {e}\")\n",
    "\n",
    "\n",
    "# Example:\n",
    "# await test_combined_rag(\"What is the vacation accrual policy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11 Â· Configuration Notes & Customization\n",
    "\n",
    "### Adapting to Your Environment\n",
    "\n",
    "| Setting | Where to Change | Notes |\n",
    "|---------|----------------|-------|\n",
    "| Certara.AI URL | `CERTARA_BASE_URL` in Â§1 | Must expose OpenAI-compatible `/v1/chat/completions` |\n",
    "| Model name | `CERTARA_MODEL` in Â§1 | Match your deployed model identifier |\n",
    "| Weaviate URL | `WEAVIATE_URL` in Â§1 | Default Weaviate port is 8080 |\n",
    "| Collection name | `WEAVIATE_COLLECTION` in Â§1 | Must match your ingested document collection |\n",
    "| Confidence threshold | `CONFIDENCE_THRESHOLD` in Â§1 | Lower = more permissive, Higher = more retries |\n",
    "| Max retries | `MAX_RETRIEVAL_RETRIES` in Â§1 | Cap on retrieval retry loops |\n",
    "| Chunks per query | `TOP_K_RESULTS` in Â§1 | More chunks = more context but higher latency |\n",
    "\n",
    "### API Endpoint Assumptions\n",
    "\n",
    "This notebook assumes your Certara.AI deployment exposes three endpoints:\n",
    "\n",
    "```\n",
    "POST /v1/chat/completions    # Standard OpenAI-compatible generation\n",
    "POST /v1/retrieve            # Vector search against Weaviate\n",
    "POST /v1/rag                 # Combined retrieve + generate\n",
    "```\n",
    "\n",
    "If your API differs, update the `retrieve_from_weaviate()` and `certara_rag_call()` functions in Â§2 to match your endpoint signatures and response formats.\n",
    "\n",
    "### Extending the Graph\n",
    "\n",
    "Common extensions you might add:\n",
    "\n",
    "- **Guardrails node** â€” check generated answers for policy compliance or PII before returning\n",
    "- **Routing to specialized sub-graphs** â€” e.g., different retrieval strategies for HR vs. Finance policies\n",
    "- **Human-in-the-loop** â€” use LangGraph's `interrupt_before` to pause before certain nodes and require approval\n",
    "- **Streaming** â€” replace `astream` with `astream_events` for token-level streaming to the UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
